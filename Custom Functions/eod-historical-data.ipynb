{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "import pytz\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import os\n",
    "\n",
    "api_key = os.environ.get('EOD_Historical_Data_API_Key')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_daily_prices(symbol: str, start_date: str, end_date: str = None, adjusted: bool = True):\n",
    "\n",
    "    # Set last date of dataset to today unless assigned during function call\n",
    "    if end_date is None:\n",
    "        end_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # Validate Start & End Date \n",
    "    # Convert strings to datetime objects\n",
    "    start_date_dtObj = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end_date_dtObj = datetime.datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "    # Validate\n",
    "    start_date_dtObj = min(start_date_dtObj, end_date_dtObj)\n",
    "    end_date_dtObj = max(start_date_dtObj, end_date_dtObj)\n",
    "\n",
    "    #OHLCV\n",
    "    # API Request to return Daily OHLCV in Pandas DataFrame\n",
    "    api_url = f'https://eodhistoricaldata.com/api/eod/{symbol}.US?from={start_date_dtObj}&to={end_date_dtObj}&api_token={api_key}&fmt=json'\n",
    "    data = requests.get(api_url).json()\n",
    "    df = pd.DataFrame(data)\n",
    "    # Add column to DataFrame containing Symbol\n",
    "    df.insert(loc=0, column='symbol', value=symbol)\n",
    "    # Format date column as datetime, and set as DataFrame index\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.set_index(\"date\", drop=False)\n",
    "\n",
    "    if adjusted:\n",
    "        # Splits\n",
    "        # API Request to return historical Splits in Pandas DataFrame\n",
    "        api_url = f'https://eodhistoricaldata.com/api/splits/{symbol}.US?api_token={api_key}&from={start_date_dtObj}&fmt=json'\n",
    "        data = requests.get(api_url).json()\n",
    "        splits = pd.DataFrame(data)\n",
    "        # Check if the DataFrame is emtpty \n",
    "        # If the DataFrame is empty, create a placeholder DF that will be used to hold split factor of 1\n",
    "        if len(splits.index) == 0:\n",
    "            splits = pd.DataFrame(data={'split': 1}, index=[start_date_dtObj])\n",
    "        # If the DataFrame is not empty, format the DataFrame to hold split factors\n",
    "        else:\n",
    "        # Format Splits for function adjustedPrice \n",
    "            splits['split'] = splits.eval(splits['split'])\n",
    "            splits['split'] = 1 / splits['split']\n",
    "            # Format date column as datetime, and set as DataFrame index\n",
    "            splits['date'] = pd.to_datetime(splits['date'])\n",
    "            splits.set_index(\"date\", drop=True, inplace=True)\n",
    "\n",
    "        # Dividends\n",
    "        # API Request to return historical Dividends in Pandas DataFrame\n",
    "        api_url = f'https://eodhistoricaldata.com/api/div/{symbol}.US?api_token={api_key}&from={start_date_dtObj}&fmt=json'\n",
    "        data = requests.get(api_url).json()\n",
    "        dividends = pd.DataFrame(data)\n",
    "        # Check if the DataFrame is emtpty \n",
    "        # If the DataFrame is empty, create a placeholder DF that will be used to hold dividend amount of 0\n",
    "        if len(dividends.index) == 0:\n",
    "            dividends = pd.DataFrame(data={'unadjustedValue': 0}, index=[start_date_dtObj])\n",
    "        # If the DataFrame is not empty, format the DataFrame to hold dividend amounts\n",
    "        else:\n",
    "        # Drop unnecessary columns\n",
    "            dividends_keep_col = ['date', 'unadjustedValue']\n",
    "            dividends = dividends[dividends_keep_col]\n",
    "            # Format date column as datetime, and set as DataFrame index\n",
    "            dividends['date'] = pd.to_datetime(dividends['date'])\n",
    "            dividends.set_index(\"date\", drop=True, inplace=True)\n",
    "\n",
    "        # Merge Splits & Dividends to OHLCV DataFrame\n",
    "        df = df.join(splits).fillna(1)  # Fill NaN values with a default split factor of 1.\n",
    "        df = df.join(dividends).fillna(0)  # Fill NaN values with a default dividend amount of 0.\n",
    "\n",
    "        # Drop existing adjusted_close\n",
    "        df.drop(['adjusted_close'], axis=1, inplace=True)\n",
    "\n",
    "        # Add columns for Adjusted OHLC\n",
    "        adjusted_daily_price(df, 'open')\n",
    "        adjusted_daily_price(df, 'high')\n",
    "        adjusted_daily_price(df, 'low')\n",
    "        adjusted_daily_price(df, 'close')\n",
    "\n",
    "        # Drop split and dividends columns\n",
    "        remove_col = ['split', 'unadjustedValue']\n",
    "        df.drop(remove_col, axis=1, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intraday_prices(symbol: str, start_date: str, end_date: str = None, interval: int = 2, adjusted: bool = True):\n",
    "\n",
    "    # Define API max period in days\n",
    "    API_MAX_PERIOD = 120\n",
    "    # Define number of minutes starting from 00:00 up to the following day\n",
    "    EOD_MINUTES = 60 * 24 - 1\n",
    "    # Define number of minutes in trading day (normal market hours)\n",
    "    TRADING_DAY_MINUTES = int(60 * 6.5)\n",
    "\n",
    "    # Set last date of dataset to today unless assigned during function call\n",
    "    if end_date is None:\n",
    "        end_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # Validate Start & End Date \n",
    "    # Convert strings to datetime objects\n",
    "    start_date_dtObj = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end_date_dtObj = datetime.datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "    # Validate\n",
    "    start_date_dtObj = min(start_date_dtObj, end_date_dtObj)\n",
    "    end_date_dtObj = max(start_date_dtObj, end_date_dtObj)\n",
    "\n",
    "    # Calculate n-number of Date Ranges with a maximum 120-day period\n",
    "    # Create a date table containing all of the dates between the start and end dates defined during function call\n",
    "    date_table = pd.date_range(start=start_date, end=end_date, tz='US/Eastern').to_frame(index=False, name='date')\n",
    "    # Add a column to the date table that will be used to group date ranges with a maximum of 120 days bewteen the first and last date\n",
    "    date_table['date_range'] = -(-(date_table.index.to_numpy() + 1) // API_MAX_PERIOD)  # Upside-down floor division to convert floor to ceiling with negation. \n",
    "    # Create a list of all of the 120-day date ranges that make up the duration defined during function call\n",
    "    date_ranges = date_table.groupby('date_range')['date'].apply(list)\n",
    "    # Define number of iterations for For Loop\n",
    "    number_of_date_ranges = np.max(date_table['date_range'])\n",
    "    # Convert the Date Ranges to only two values: the start and end datetime values, converted to Unix UTC, and perform API call\n",
    "    # Create an empty list to hold the DataFrame results of each date range.\n",
    "    dfs = []\n",
    "    for date_range in range(1, number_of_date_ranges + 1):  # For Loop for n-number of Date Ranges\n",
    "        date_range_start_date = np.min(date_ranges[date_range])  # Start of Date Range\n",
    "        date_range_end_date = np.max(date_ranges[date_range]) + timedelta(minutes=EOD_MINUTES)  # End of Date Range\n",
    "        utc_tz = pytz.timezone('UTC')  # Define UTC timezone\n",
    "        date_range_start_date_utc = utc_tz.normalize(date_range_start_date)  # Convert date range start date to UTC timezone\n",
    "        date_range_end_date_utc = utc_tz.normalize(date_range_end_date)  # Convert date range end date to UTC timezone\n",
    "        date_range_start_date_unix_utc = int(date_range_start_date_utc.timestamp())  # Convert date range start date to Unix UTC\n",
    "        date_range_end_date_unix_utc = int(date_range_end_date_utc.timestamp())  # Convert date range end date to Unix UTC\n",
    "        # API Request to return Pandas DataFrame\n",
    "        api_url = f'https://eodhistoricaldata.com/api/intraday/{symbol}.US?api_token={api_key}&interval=1m&fmt=json&from={date_range_start_date_unix_utc}&to={date_range_end_date_unix_utc}'\n",
    "        data = requests.get(api_url).json()\n",
    "        df = pd.DataFrame(data)\n",
    "        dfs.append(df)  # Append DataFrames\n",
    "    # Return a single DataFrame conatining all Date Ranges\n",
    "    df = pd.concat(dfs) \n",
    "\n",
    "    # Sort and reset index to verify chronological order\n",
    "    df.sort_values(by='timestamp', ascending=True, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Add column: convert Unix UTC timestamp to Eastern timezone\n",
    "    unix_utc = pd.to_datetime(df['timestamp'], unit='s') \n",
    "    df['datetime_est'] = unix_utc.dt.tz_localize('UTC')  # UTC timezone aware\n",
    "    df['datetime_est'] = df['datetime_est'].dt.tz_convert('US/Eastern')  # Convert to Eastern timezone\n",
    "    df['datetime_est'] = df['datetime_est'].dt.tz_localize(None)  # Return timezone-unaware datetime\n",
    "    \n",
    "    # Add column to DataFrame, in 1st position, containing truncated Date (EST)\n",
    "    df.insert(loc=0, column='date_est', value=df['datetime_est'].dt.date)\n",
    "\n",
    "    # Set the daily close price to the open of the next row (16:00 est) \n",
    "    # ***Intraday Closing Data returned by EOD Histroical Data does not match Daily Closing Data. The Open of the following row (16:00 est) matches***\n",
    "    close_mask = df['datetime_est'].dt.time == datetime.time(15, 59, 0)\n",
    "    df['close'] = df['open'].shift(-1).where(close_mask, other=df['close'])\n",
    "\n",
    "    # Calculate 'Trading Session' for all data points (vectorized)\n",
    "    # Trading Hours\n",
    "    market_start = datetime.time(9, 30, 0)\n",
    "    market_end = datetime.time(16, 0, 0) \n",
    "    # Conditions\n",
    "    conditions = [\n",
    "                    df['datetime_est'].dt.time < market_start,\n",
    "                    df['datetime_est'].dt.time >= market_end\n",
    "                ]\n",
    "    # Trading Sessions\n",
    "    values = ['Pre-Market', 'After Horus']\n",
    "    # Add column for Trading Session\n",
    "    df['trading_session'] = np.select(conditions, values, default='Market')\n",
    "   \n",
    "    # Filter for Market hours\n",
    "    df = df[df['trading_session'] == 'Market']\n",
    "\n",
    "    # Define Interval Groups\n",
    "    position = df.columns.get_loc('datetime_est')\n",
    "    elapsed = df.iloc[0:, position] - df.iat[0, position]\n",
    "    minutes_elapsed = (elapsed.dt.seconds/60) + 1\n",
    "    df['interval_group'] = -(-minutes_elapsed // (TRADING_DAY_MINUTES/interval))  # Upside-down floor division to convert floor to ceiling with negation. \n",
    "\n",
    "    # Aggregate Data using Interval Groups\n",
    "    agg_dict = {\n",
    "        'open': 'first',\n",
    "        'high': np.max,\n",
    "        'low': np.min,\n",
    "        'close': 'last',\n",
    "        'volume': np.sum\n",
    "    }\n",
    "    df = df.groupby(['date_est', 'interval_group']).agg(agg_dict).reset_index()\n",
    "\n",
    "    # Add column to DataFrame, in 1st position, containing Symbol\n",
    "    df.insert(loc=0, column='symbol', value=symbol)\n",
    "\n",
    "    # Set Index\n",
    "    df.set_index('date_est', inplace=True)\n",
    "\n",
    "    if adjusted:\n",
    "        # Calculate the Adusted OHLC as a percentage on the daily timeframe\n",
    "        adj_df = get_daily_prices(symbol, start_date, end_date, adjusted=True)\n",
    "        adjusted_percent(adj_df, 'open')\n",
    "        adjusted_percent(adj_df, 'high')\n",
    "        adjusted_percent(adj_df, 'low')\n",
    "        adjusted_percent(adj_df, 'close')\n",
    "        # Drop unnecessary columns so the DataFrame only contains percentages\n",
    "        adj_df = adj_df.loc[:,adj_df.columns.str.endswith('_percent')]\n",
    "        # Merge Adjusted Percentages to intraday OHLCV DataFrame\n",
    "        df = df.join(adj_df)\n",
    "\n",
    "        # Calculate Adjusted Prices\n",
    "        intraday_adjusted_prices(df, 'open')\n",
    "        intraday_adjusted_prices(df, 'high')\n",
    "        intraday_adjusted_prices(df, 'low')\n",
    "        intraday_adjusted_prices(df, 'close')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjusted_price(df, column):\n",
    "\n",
    "    \"\"\"\n",
    "    https://joshschertz.com/2016/08/27/Vectorizing-Adjusted-Close-with-Python/ \n",
    "    Vectorized approach for calculating the adjusted prices for the\n",
    "    specified column in the provided DataFrame. This creates a new column\n",
    "    called 'adj_<column name>' with the adjusted prices. This function requires\n",
    "    that the DataFrame have columns with dividend and split_ratio values.\n",
    "\n",
    "    :param df: DataFrame with raw prices along with dividend and split_ratio values\n",
    "    :param column: String of which price column should have adjusted prices created for it\n",
    "    :return: DataFrame with the addition of the adjusted price column\n",
    "    \"\"\"\n",
    "    \n",
    "    adj_column = 'adjusted_' + column\n",
    "\n",
    "    # Reverse the DataFrame order, sorting by date in descending order\n",
    "    df.sort_index(ascending=False, inplace=True)\n",
    "\n",
    "    price_col = df[column].to_numpy()\n",
    "    split_col = df['split'].to_numpy()\n",
    "    dividend_col = df['unadjustedValue'].to_numpy()\n",
    "    adj_price_col = np.zeros(len(df.index))\n",
    "    adj_price_col[0] = price_col[0]\n",
    "\n",
    "    for i in range(1, len(price_col)):\n",
    "        adj_price_col[i] = round((adj_price_col[i-1] + adj_price_col[i-1] * (((price_col[i] * split_col[i-1]) - price_col[i-1] - dividend_col[i-1]) / price_col[i-1])), 4)\n",
    "\n",
    "    df[adj_column] = adj_price_col\n",
    "\n",
    "    # Change the DataFrame order back to dates ascending\n",
    "    df.sort_index(ascending=True, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjusted_percent(df, column):\n",
    "\n",
    "    # Data Validation\n",
    "    if column.startswith('adjusted_'):\n",
    "        adj_column = column\n",
    "        column = column[column.find(\"_\")+1:].split()[0]\n",
    "    else:\n",
    "        adj_column = 'adjusted_' + column\n",
    "        column = column\n",
    "\n",
    "    price_col = df[column].to_numpy()\n",
    "    adj_price_col = df[adj_column].to_numpy()\n",
    "    adj_percent = np.zeros(len(df.index))\n",
    "\n",
    "    for i in range(0, len(price_col)):\n",
    "        adj_percent[i] = adj_price_col[i] / price_col[i]\n",
    "\n",
    "    df['adjusted_' + column + '_percent'] = adj_percent\n",
    "\n",
    "    return df\n",
    "\n",
    "def intraday_adjusted_prices(df, column):\n",
    "\n",
    "    adj_column = 'adjusted_' + column\n",
    "    adj_percent_column = 'adjusted_' + column + '_percent'\n",
    "\n",
    "    price_col = df[column].to_numpy()\n",
    "    adj_percent = df[adj_percent_column].to_numpy()\n",
    "    adj_price = np.zeros(len(df.index))\n",
    "\n",
    "    for i in range(0, len(price_col)):\n",
    "        adj_price[i] = round(adj_percent[i] * price_col[i],4)\n",
    "\n",
    "    df.drop([adj_percent_column], axis=1, inplace=True)\n",
    "\n",
    "    df[adj_column] = adj_price\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technical Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sma(df, column: str, period: int):\n",
    "\n",
    "    # Calculate SMA\n",
    "    sma = df[column].rolling(window=period, min_periods=period).mean()\n",
    "\n",
    "    # Add SMA column to DataFrame\n",
    "    col_header = column + \"_sma_\" + str(period)\n",
    "    df[col_header] = sma\n",
    "\n",
    "    return sma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ema(df, column: str, period: int):\n",
    "\n",
    "    # Calculate EMA\n",
    "    ema = df[column].ewm(span=period, adjust=False, min_periods=period).mean()\n",
    "\n",
    "    # Add EMA column to DataFrame\n",
    "    col_header = column + \"_ema_\" + str(period)\n",
    "    df[col_header] = ema\n",
    "\n",
    "    return ema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arnaud Legoux Moving Average\n",
    "Offset: You can set the offset in decimals between the level of 0 and 1. A setting of 0.99 makes the ALMA extremely responsive, while a value of 0.01 makes it very smooth.\n",
    "\n",
    "Sigma: A setting of 6 makes the filter rather large while a smaller sigma setting makes it more focused. According to Mr. Legoux, a sigma value of 6 is said to offer good performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alma(df, column: str, period: int, offset: float = 0.85, sigma: float = 6.0):\n",
    "\n",
    "    # Convert column to Numpy Array\n",
    "    data = df[column].to_numpy()\n",
    "\n",
    "    # ALMA inputs\n",
    "    m = np.floor(offset * (period-1))\n",
    "    s = period / sigma \n",
    "    alma = np.zeros(data.shape)\n",
    "    w_sum = np.zeros(data.shape)\n",
    "\n",
    "    # Calculate ALMA for each row\n",
    "    for i in range(len(data)):\n",
    "        if i < period-1:\n",
    "            alma[i] = np.nan\n",
    "        else:\n",
    "            for j in range(period):\n",
    "                w = np.exp(-(j-m)*(j-m)/(2*s*s))\n",
    "                alma[i] += data[i + 1 - period + j] * w\n",
    "                w_sum[i] += w\n",
    "            alma[i] = alma[i] / w_sum[i]\n",
    " \n",
    "    # Add ALMA column to DataFrame\n",
    "    col_header = column + \"_alma_\" + str(period)\n",
    "    df[col_header] = alma\n",
    "    \n",
    "    return alma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bollinger Bands\n",
    "Short Term: 10 day MA, 1.5 std dev\n",
    "\n",
    "Medium Term (most commonly used): 20 day MA, 2 std dev\n",
    "\n",
    "Long Term: 50 day MA, 2.5 std dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bollingerBands(df, column: str, ma_type: str = 'alma', period: int = 20, std: int = 2, plot: bool = True):\n",
    "\n",
    "    data = df[column]\n",
    "    ma_type = ma_type.lower()\n",
    "\n",
    "    # Calculate moving average\n",
    "    if ma_type == 'alma':\n",
    "        # Use Arnaud Legoux moving average\n",
    "        ma = alma(df, column, period)\n",
    "    elif ma_type == 'ema':\n",
    "        # Use exponential moving average\n",
    "        ma = ema(df, column, period)\n",
    "    else :\n",
    "        # Use simple moving average\n",
    "        ma = sma(df, column, period)\n",
    "\n",
    "    # Calculate rolling Standard Deviation\n",
    "    rstd = data.rolling(window=period, min_periods=period).std()\n",
    "\n",
    "    # Moving Average column header\n",
    "    col_header = column + '_' + ma_type + '_' + str(period)\n",
    "\n",
    "    # Add columns to DataFrame with Bollinger Bands moving average & period in header\n",
    "    df['bb_upper_' + col_header] = ma + (rstd * std)\n",
    "    df['bb_lower_' + col_header] = ma - (rstd * std)\n",
    "\n",
    "    # Show Visual\n",
    "    if plot:\n",
    "        plt.style.use('fivethirtyeight')\n",
    "        plt.rcParams['figure.figsize'] = (20, 10)\n",
    "        #Determine color of price line depending on dataset trend; green for positive growth, red for negative growth.\n",
    "        if df.iloc[0][column] > df.iloc[-1][column]: \n",
    "            data.plot(label=column, color='#E45756', linewidth=3)\n",
    "        else:\n",
    "            data.plot(label=column, color='#54A24B', linewidth=2)\n",
    "        df['bb_upper_' + col_header].plot(label='bb_upper_' + col_header, linestyle='--', linewidth=1, color='black')\n",
    "        df[col_header].plot(label=col_header, linestyle='--', linewidth=1.2, color='grey')\n",
    "        df['bb_lower_' + col_header].plot(label='bb_lower_' + col_header, linestyle='--', linewidth=1, color='black')\n",
    "        plt.legend(loc='upper left')\n",
    "        plt.title(df['symbol'].iloc[0].upper())\n",
    "        plt.show()\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keltnerChannels(df, column: str, ma_type: str = 'alma', kc_period: int = 20, kc_multiplier: int = 2, atr_period: int = 10, plot: bool = True):\n",
    "\n",
    "    data = df[column]\n",
    "    ma_type = ma_type.lower()\n",
    "\n",
    "    # Calculate moving average\n",
    "    if ma_type == 'alma':\n",
    "        # Use Arnaud Legoux moving average\n",
    "        ma = alma(df, column, kc_period)\n",
    "    elif ma_type == 'ema':\n",
    "        # Use exponential moving average\n",
    "        ma = ema(df, column, kc_period)\n",
    "    else :\n",
    "        # Use simple moving average\n",
    "        ma = sma(df, column, kc_period)\n",
    "\n",
    "    # Calculate ATR\n",
    "    kc_atr = atr(df, adjusted=True, ma_type=ma_type, period=atr_period)\n",
    "\n",
    "    # Moving Average column header\n",
    "    col_header = column + '_' + ma_type + '_' + str(kc_period)\n",
    "\n",
    "    # Add columns to DataFrame with Bollinger Bands moving average & period in header\n",
    "    df['kc_upper_' + col_header] = ma + (kc_atr * kc_multiplier)\n",
    "    df['kc_lower_' + col_header] = ma - (kc_atr * kc_multiplier)\n",
    "\n",
    "    # Show Visual\n",
    "    if plot:\n",
    "        plt.style.use('fivethirtyeight')\n",
    "        plt.rcParams['figure.figsize'] = (20, 10)\n",
    "        #Determine color of price line depending on dataset trend; green for positive growth, red for negative growth.\n",
    "        if df.iloc[0][column] > df.iloc[-1][column]: \n",
    "            data.plot(label=column, color='#E45756', linewidth=3)\n",
    "        else:\n",
    "            data.plot(label=column, color='#54A24B', linewidth=2)\n",
    "        df['kc_upper_' + col_header].plot(label='kc_upper_' + col_header, linestyle='--', linewidth=1, color='black')\n",
    "        df[col_header].plot(label=col_header, linestyle='--', linewidth=1.2, color='grey')\n",
    "        df['kc_lower_' + col_header].plot(label='kc_lower_' + col_header, linestyle='--', linewidth=1, color='black')\n",
    "        plt.legend(loc='upper left')\n",
    "        plt.title(df['symbol'].iloc[0].upper())\n",
    "        plt.show()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rsi(df, column: str, period: int = 14, ma_type: str = 'alma'):\n",
    "\n",
    "    delta = df[[column]].diff()\n",
    "    ma_type = ma_type.lower()\n",
    "\n",
    "    # Make two series: one for lower closes and one for higher closes\n",
    "    up = delta.clip(lower=0)\n",
    "    down = -1 * delta.clip(upper=0)\n",
    "    \n",
    "    # Calculate moving average\n",
    "    if ma_type == 'alma':\n",
    "        # Use Arnaud Legoux moving average\n",
    "        ma_up = alma(up, column, period)\n",
    "        ma_down = alma(down, column, period)\n",
    "    elif ma_type == 'ema':\n",
    "        # Use exponential moving average\n",
    "        ma_up = ema(up, column, period)\n",
    "        ma_down = ema(down, column, period)\n",
    "    else :\n",
    "        # Use simple moving average\n",
    "        ma_up = sma(up, column, period)\n",
    "        ma_down = sma(down, column, period)\n",
    "\n",
    "    # Calculate RSI\n",
    "    rs = ma_up / ma_down\n",
    "    rsi = 100 - (100/(1 + rs))\n",
    "\n",
    "    # Moving Average column header\n",
    "    col_header = column + '_' + ma_type + '_' + str(period)\n",
    "\n",
    "    # Add column to DataFrame with RSI moving average & period in header\n",
    "    df['rsi_' + col_header] = rsi\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atr(df, adjusted: bool = True, ma_type: str = 'alma', period: int = 10):\n",
    "\n",
    "    ma_type = ma_type.lower()\n",
    "\n",
    "    # Assign HLC variables to DataFrame columns\n",
    "    if adjusted:\n",
    "        high = df['adjusted_high']\n",
    "        low = df['adjusted_low']\n",
    "        close = df['adjusted_close']\n",
    "    else:\n",
    "        high = df['high']\n",
    "        low = df['low']\n",
    "        close = df['close']\n",
    "        \n",
    "    # Calculate True Ranges\n",
    "    tr1 = high - low\n",
    "    tr2 = np.abs(high - close.shift())\n",
    "    tr3 = np.abs(low - close.shift())\n",
    "\n",
    "    # Find Max of True Ranges\n",
    "    df['tr'] = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)\n",
    "\n",
    "    # Calculate ATR\n",
    "    if ma_type == 'alma':\n",
    "        # Use Arnaud Legoux moving average\n",
    "        atr = alma(df, 'tr', period)\n",
    "    elif ma_type == 'ema':\n",
    "        # Use exponential moving average\n",
    "        atr = ema(df, 'tr', period)\n",
    "    else :\n",
    "        # Use simple moving average\n",
    "        atr = sma(df, 'tr', period)\n",
    "\n",
    "    # Drop True Range column\n",
    "    df.drop(columns='tr', inplace=True)\n",
    " \n",
    "    return atr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sp500():\n",
    "\n",
    "    # Read table from Wiki\n",
    "    sp500 = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')[0]\n",
    "\n",
    "    # Sort\n",
    "    sp500 = sp500.sort_values(by='Symbol')\n",
    "\n",
    "    # Convert to list\n",
    "    sp500 = sp500.Symbol.to_list()\n",
    "\n",
    "    # Convert Non-Class A Stocks to readable format\n",
    "    sp500 = [i.replace('.','-') for i in sp500]\n",
    "\n",
    "    return sp500"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1f23e7a4be69fe68ef5b7dddf2a6fcce310e5afb496bdf0fadddae054551b389"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
